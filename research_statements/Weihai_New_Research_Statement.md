# Research Statement

**Weihai Shen**

The infrastructure powering our digital world—from financial markets to social networks to cloud platforms—depends fundamentally on distributed systems that must process millions of transactions per second while never compromising data consistency. Yet conventional approaches to building such systems face an *apparent impossibility*: achieving both high performance and strong correctness guarantees simultaneously. Industry systems like Google's Spanner achieve strong consistency but sacrifice performance, managing only thousands of transactions per second despite running on powerful hardware. Conversely, many high-performance systems achieve speed by weakening consistency guarantees, introducing the risk of data corruption or loss.

My research challenges this fundamental trade-off by developing distributed systems that achieve exceptional performance without compromising correctness through principled use of speculative execution. *The key insight driving my work is that servers in traditional replicated systems spend most of their time waiting*—for coordination between partitions, for replication among replicas, and for failure recovery. This waiting represents wasted computational resources that can be systematically reclaimed through careful orchestration of speculative execution across system layers.

Rather than viewing speculation as an optimization technique, *I architect systems where speculation becomes a fundamental design principle embedded from the ground up*. My approach has demonstrated that *perceived trade-offs between performance and consistency often reflect design limitations rather than fundamental constraints*, opening new possibilities for infrastructure systems that can scale to serve billions of users while maintaining the strictest correctness guarantees.

## Breaking the Replication Performance Barrier

The first major challenge I tackled was the performance bottleneck in fault-tolerant transaction systems. Traditional approaches tightly couple transaction execution with replication protocols: systems like Spanner use Paxos to replicate each step in transaction execution, forcing cores to wait for network communication before processing subsequent transactions. This conservative approach achieves strong consistency but at enormous performance cost—only several thousand transactions per second despite powerful hardware.

I developed **Rolis**, a replicated transaction system that fundamentally reimagines how replication and concurrency control interact. *The key insight is that these are orthogonal concerns that have been artificially coupled in existing systems*. Rolis introduces an "execute-replicate-replay" architecture where transactions execute speculatively on the leader while being asynchronously replicated to followers, who then deterministically replay the same sequence of operations.

*The technical breakthrough lies in the architecture's clean separation of concerns*. The leader optimizes for transaction throughput using established multi-core concurrency control techniques, while independent per-thread Paxos streams handle replication without blocking transaction execution. Each thread creates its own replication stream, maintaining the serialization order required for deterministic replay while *eliminating cross-thread coordination overhead*.

This design choice enables unprecedented performance: *Rolis achieves throughput within 5% of non-replicated single-node databases while providing identical fault tolerance guarantees* as traditional replicated systems. On modern multi-core hardware, Rolis processes over one million transactions per second with linear scalability—*nearly three orders of magnitude improvement over conventional replicated systems*. This result demonstrates that *the traditional performance-consistency trade-off stems from architectural limitations, not fundamental constraints*.

## Scaling Speculation to Planetary Distribution

Building on Rolis's success, I next addressed an even more challenging problem: enabling speculation across geo-distributed, multi-shard systems where hundreds of thousands of shards span multiple continents. Here, network latencies are orders of magnitude higher and the coordination complexity grows exponentially with geographic scale and shard count.

The fundamental challenge is that traditional approaches to geo-distributed systems face irreconcilable scalability problems. Systems like Spanner apply Paxos across wide-area networks to replicate critical coordination steps, but *the resulting latencies make speculation nearly impossible*—by the time conflicts are detected, speculative work has proceeded too far to roll back efficiently. Recent systems like COCO achieve higher throughput through epoch-based batching, but introduce *catastrophic failure modes*: the entire system blocks every 10ms during synchronization and halts completely when any single shard fails.

I developed **Mako** to demonstrate that speculation principles can extend to planetary-scale distribution through careful dependency tracking and conflict isolation. *The key innovation is a vector watermark protocol that enables fine-grained speculation across both geographic replicas and logical shards while maintaining full serializability*. Unlike epoch-based systems that create global synchronization points, Mako's watermarks allow each shard to make independent progress, *preventing cascading failures and eliminating system-wide blocking*.

*Mako's technical breakthrough lies in multidimensional dependency tracking* using vector timestamps that capture both logical transaction ordering within shards and physical propagation delays across geographic regions. Each shard maintains vector clocks tracking causal dependencies with respect to all other shards, enabling *precise conflict detection without global coordination*. When conflicts arise, Mako performs *surgical rollbacks that affect only the specific conflicting transactions* while allowing all other work to proceed.

The system achieves *geographic latency masking through aggressive speculation*: transactions begin executing using locally cached data while coordination messages traverse continents. When fresh data arrives and conflicts are detected, only the affected transactions require rollback—*the system avoids the cascading failures that plague existing geo-distributed systems*.

Our evaluation demonstrates that *Mako achieves 3-5× throughput improvements over Calvin and Spanner while maintaining robust performance under failure conditions*. Under high-contention workloads where traditional systems collapse due to coordination overhead, Mako maintains stable performance by isolating conflict effects. Most significantly, *individual shard failures or high-latency events do not affect overall system performance*, proving that speculation can provide both performance enhancement and fault tolerance at geographic scale.

Mako's success validates a broader principle: *speculation techniques that eliminate idle waiting in single-machine systems can be systematically extended to eliminate coordination overhead across planetary-scale distribution*. This suggests *speculation may provide a general architectural solution to coordination bottlenecks across all scales of distributed computing*.

I have also contributed to systems programming accessibility through collaborative work on DepFast, a framework that enables developers to write distributed systems using synchronous programming constructs while maintaining asynchronous performance. This work complements my core research by addressing the practical challenges of building the complex systems that speculation techniques enable.

## A New Foundation for Distributed Computing

My work with Rolis and Mako has demonstrated that *speculation can break through fundamental performance barriers that have constrained distributed systems for decades*. However, these successes point toward an even more ambitious goal: *establishing speculation as a foundational design principle that transforms how we build all distributed systems*, from consensus protocols to serverless platforms to distributed machine learning frameworks.

*The key insight emerging from my research is that coordination bottlenecks pervade distributed computing precisely because existing systems are built around conservative synchronization assumptions*. Servers wait for explicit confirmation before proceeding, protocols require multiple round-trips to establish agreement, and failures trigger expensive recovery procedures that block progress across entire systems. Yet in most cases, *this waiting is unnecessary*—systems could proceed speculatively and roll back only when actual conflicts arise.

**Speculation as a System Design Principle**: My immediate goal is to *distill the speculation patterns from Rolis and Mako into a unified design framework* that can be systematically applied across diverse system domains. This framework must capture the essential elements—*multidimensional dependency tracking, fine-grained conflict detection, surgical rollback mechanisms*—while abstracting away domain-specific implementation details. The framework should enable systems designers to *reason about speculation at the architectural level*, making principled decisions about where to apply speculation and how to coordinate speculative execution across system components.

Such a framework would *enable speculation to transform domains far beyond database systems*. Distributed consensus protocols could *pipeline agreement phases speculatively* rather than waiting for each step to complete. Serverless platforms could *pre-execute functions based on predicted request patterns*, rolling back only when predictions prove incorrect. Distributed machine learning systems could *speculatively begin training iterations while synchronization is still in progress*, dramatically reducing the idle time that currently limits scalability.

**Programming Language Support for Speculation**: *The current barrier to widespread adoption of speculation techniques is their complexity*: developers must manually implement dependency tracking, conflict detection, and rollback mechanisms, making speculation accessible only to systems experts. I plan to develop programming language constructs that *make speculation transparent to application developers* while ensuring correctness through compiler analysis and runtime support.

This language-level support would *automatically insert speculation barriers based on data dependency analysis*, generate efficient rollback code, and optimize speculative execution patterns based on dynamic program behavior. The result would be *democratization of high-performance distributed system development*, enabling a much broader community of developers to build systems that achieve the performance benefits of speculation without requiring deep expertise in coordination protocols.

**Hardware-Software Co-Design for Distributed Speculation**: Current processor architectures include sophisticated speculation support designed for single-threaded performance—branch prediction, out-of-order execution, speculative memory operations—but *these mechanisms are poorly suited to the longer-lived, more complex speculation patterns that arise in distributed systems*. I plan to explore architectural extensions that directly support distributed speculation patterns, potentially including *hardware primitives for distributed dependency tracking*, specialized memory consistency models for speculative operations, and *architectural support for efficient distributed rollback*.

Such architectural support could *enable speculation techniques that are impractical with current general-purpose processors*, opening new possibilities for high-performance distributed algorithms that fully exploit the parallelism available in modern datacenter environments.

**Speculation-Native Algorithm Design**: Rather than retrofitting existing algorithms to use speculation, I aim to *design new distributed algorithms built from first principles around speculative execution*. These algorithms would *embrace speculation as a fundamental design element*, potentially achieving both better performance and simpler implementations than current approaches.

For example, *speculation-native consensus algorithms could allow multiple agreement instances to proceed in parallel* while using dependency tracking to maintain safety properties. Distributed scheduling algorithms could *make resource allocation decisions speculatively*, rolling back only when conflicts with other schedulers are detected. Such algorithms could achieve *higher throughput and lower latency than current approaches while maintaining identical correctness guarantees*.

The convergence of several trends—increasing computational demands, evolving hardware architectures, and growing system complexity—creates an unprecedented opportunity to *reimagine distributed system design from first principles*. My research program will *establish speculation as the foundation for this transformation*, ultimately enabling a new generation of infrastructure systems that can scale to serve planetary-scale applications while maintaining the strongest possible correctness guarantees. *The ultimate vision is a world where high-performance distributed computing becomes accessible to all developers, not just systems experts*, democratizing the infrastructure capabilities needed for the next generation of applications.